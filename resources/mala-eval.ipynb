{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13acf52f-7b24-478c-9c62-49ccb8f93237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import csv\n",
    "\n",
    "BASE_PROMPT = \"\"\"Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers. The AI is supposed to answer test questions from the human with short responses saying just the answer and nothing else.\n",
    "\n",
    "Human: {instruction}\n",
    "\n",
    "Assistant [Amharic] : \"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import datasets\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pandas as pd\n",
    "tqdm_notebook.pandas()\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftModel\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\"daryl149/llama-2-7b-hf\")\n",
    "# base_model.resize_token_embeddings(260164)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('MaLA-LM/mala-500')\n",
    "# model = PeftModel.from_pretrained(base_model, 'MaLA-LM/mala-500')\n",
    "\n",
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.utils import is_accelerate_available, is_bitsandbytes_available\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "ALPACA_TEMPLATE = (\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides \"\n",
    "    \"further context. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_adapted_hf_generation_pipeline(\n",
    "    base_model_name,\n",
    "    lora_model_name,\n",
    "    temperature: float = 0,\n",
    "    top_p: float = 1.,\n",
    "    max_tokens: int = 50,\n",
    "    batch_size: int = 16,\n",
    "    device: str = \"cuda\",\n",
    "    load_in_8bit: bool = True,\n",
    "    generation_kwargs: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a huggingface model & adapt with PEFT.\n",
    "    Borrowed from https://github.com/tloen/alpaca-lora/blob/main/generate.py\n",
    "    \"\"\"\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        if not is_accelerate_available():\n",
    "            raise ValueError(\"Install `accelerate`\")\n",
    "    if load_in_8bit and not is_bitsandbytes_available():\n",
    "            raise ValueError(\"Install `bitsandbytes`\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    task = \"text-generation\"\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            load_in_8bit=load_in_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model.resize_token_embeddings(260164)\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "        )\n",
    "\n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_in_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generation_kwargs = generation_kwargs if generation_kwargs is not None else {}\n",
    "    config = GenerationConfig(\n",
    "        max_new_tokens=100,  # The maximum numbers of tokens to generate\n",
    "        seed=42,  # seed value for reproducibility\n",
    "        do_sample=True,  # Whether or not to use sampling; use greedy decoding otherwise.\n",
    "        min_length=None,  # The minimum length of the sequence to be generated\n",
    "        use_cache=True,  # [optional] Whether or not the model should use the past last key/values attentions\n",
    "        top_p=1.0,  # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "        temperature=1.0,  # [optional] The value used to modulate the next token probabilities.\n",
    "        top_k=5,  # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "        repetition_penalty=5.0,  # The parameter for repetition penalty. 1.0 means no penalty.\n",
    "        length_penalty=1,  # [optional] Exponential penalty to the length used with beam-based generation.\n",
    "        enable_azure_content_safety=False,  # Enable safety check with Azure content safety API\n",
    "        enable_sensitive_topics=False,  # Enable check for sensitive topics using AuditNLG APIs\n",
    "        enable_saleforce_content_safety=False,\n",
    "            **generation_kwargs,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        task,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=4, # TODO: make a parameter\n",
    "        generation_config=config,\n",
    "        framework=\"pt\",\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfb6aa5-0c99-41ce-b7a1-bdbf86f9ddda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mload_adapted_hf_generation_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdaryl149/llama-2-7b-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMaLA-LM/mala-500\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [optional] The value used to modulate the next token probabilities.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m hf_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misrael/JOPUjJHxWmI5x\u001b[39m\u001b[38;5;124m\"\u001b[39m,use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_iOFAkpEmlEodGfIbiIchEBogNGLgvJVrQI\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m BASE_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBelow is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers. The AI is supposed to answer test questions from the human with short responses saying just the answer and nothing else.\u001b[39m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mHuman: \u001b[39m\u001b[38;5;132;01m{instruction}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mAssistant [Amharic] : \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 84\u001b[0m, in \u001b[0;36mload_adapted_hf_generation_pipeline\u001b[0;34m(base_model_name, lora_model_name, temperature, top_p, max_tokens, batch_size, device, load_in_8bit, generation_kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;241m260164\u001b[39m)\n\u001b[1;32m     91\u001b[0m     model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     92\u001b[0m         model,\n\u001b[1;32m     93\u001b[0m         lora_model_name,\n\u001b[1;32m     94\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     95\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca-lora/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca-lora/lib/python3.8/site-packages/transformers/modeling_utils.py:3682\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3678\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3679\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3680\u001b[0m         }\n\u001b[1;32m   3681\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3682\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3683\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3684\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3685\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3686\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3687\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3688\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3689\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[1;32m   3690\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3691\u001b[0m             )\n\u001b[1;32m   3692\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3694\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "pipe = load_adapted_hf_generation_pipeline(\n",
    "    base_model_name=\"daryl149/llama-2-7b-hf\",\n",
    "    lora_model_name=\"MaLA-LM/mala-500\",\n",
    "    top_p=1.0,  # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "    temperature=1.0,  # [optional] The value used to modulate the next token probabilities.\n",
    "    )\n",
    "hf_dataset = datasets.load_dataset(\"israel/JOPUjJHxWmI5x\",use_auth_token=\"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\")['test']\n",
    "BASE_PROMPT = \"\"\"Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers. The AI is supposed to answer test questions from the human with short responses saying just the answer and nothing else.\n",
    "\n",
    "Human: {instruction}\n",
    "\n",
    "Assistant [Amharic] : \"\"\"\n",
    "def f(batch):\n",
    "    return {'response':[pipe(BASE_PROMPT.format(instruction=f\"{instruction}\\n{input}\"))[0]['generated_text'][len(BASE_PROMPT.format(instruction=f\"{instruction}\\n{input}\")):] for instruction,input in zip(batch['instruction'],batch['input'])]}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0679bb84-6e8f-4141-8d45-ff960fa1d7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'hello\\'i i ai, ko te he e ka ki ahor.\\n Kaoreakaako mo me take o? Heaaro ano mai nei auaena; na: \"\" (hewhero Na -'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53a48f4-958a-47c6-862e-5825fa8da39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azime/anaconda3/envs/alpaca-lora/lib/python3.8/site-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = datasets.load_dataset(\"israel/MT-llama\",use_auth_token=\"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\")['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2354a3cd-183d-42d1-85aa-417a86099acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'የተሰጠውን ጽሁፍ  ከ አማርኛ ቋንቋ ወደ English ቋንቋ ተርጉም።',\n",
       " 'input': '\"አሁን የስኳር በሽተኛ ያልነበሩ አሁን ግን የሆኑ የ4-ወር-ዕድሜ ያላቸው አይጦች አሉን፣ አለ። \"',\n",
       " 'output': '\"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.',\n",
       " 'prompt_header': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.',\n",
       " 'datasource': 'amharic_mt amh-eng'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e88098-179b-4495-ab60-d70bddb98624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  የተሰጠውን ጽሁፍ  ከ አማርኛ ቋንቋ ወደ English ቋንቋ ተርጉም። \\'We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\\'\\n - I am not happy about it but there\\'s nothing you can do except wait for me at home and then go back tomorrow or next day if possible.    ( You will find some of my past postings on this topic )       *******     If your father had been in his right mind, would him being ill make any difference?           Doctors don’t always know what causes sicknesses so how could they say why their treatment didn` t work when many factors'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"\"\" Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  የተሰጠውን ጽሁፍ  ከ አማርኛ ቋንቋ ወደ English ቋንቋ ተርጉም። 'We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650319d-a5fd-4409-89ed-bd0747c8f6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872ee46ff9084c2f8b6fa6b23725d9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset.map(lambda batch:f(batch) ,batch_size=8, batched=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86deb63c-c7bd-4899-b57c-4b8cfd7cfa77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca-lora",
   "language": "python",
   "name": "alpaca-lora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
